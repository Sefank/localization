{
  "noInstanceSelected": "未选择模型实例",
  "resetToDefault": "重置",
  "showAdvancedSettings": "显示高级设置",
  "showAll": "全部",
  "basicSettings": "基础",
  "configSubtitle": "加载或保存预设并尝试模型参数覆盖",
  "inferenceParameters/title": "预测参数",
  "inferenceParameters/info": "尝试影响预测的参数。",
  "generalParameters/title": "通用",
  "samplingParameters/title": "采样",
  "basicTab": "基础",
  "advancedTab": "高级",
  "advancedTab/title": "🧪 高级配置",
  "advancedTab/expandAll": "展开所有",
  "advancedTab/overridesTitle": "配置覆盖",
  "advancedTab/noConfigsText": "您没有未保存的更改 - 编辑上方值以在此处查看覆盖。",
  "loadInstanceFirst": "加载模型以查看可配置参数",
  "noListedConfigs": "无可配置参数",
  "generationParameters/info": "尝试影响文本生成的基础参数。",
  "loadParameters/title": "加载参数",
  "loadParameters/description": "控制模型初始化和加载到内存的方式的设置。",
  "loadParameters/reload": "重新加载以应用更改",
  "loadParameters/reload/error": "重新加载模型失败",
  "discardChanges": "放弃更改",
  "loadModelToSeeOptions": "加载模型以查看选项",
  "schematicsError.title": "配置模式在以下字段中包含错误：",
  "manifestSections": {
    "structuredOutput/title": "结构化输出",
    "speculativeDecoding/title": "推测性解码",
    "sampling/title": "采样",
    "settings/title": "设置",
    "toolUse/title": "工具使用",
    "promptTemplate/title": "提示模板"
  },

  "llm.prediction.systemPrompt/title": "系统提示",
  "llm.prediction.systemPrompt/description": "使用此字段向模型提供背景指令，如一套规则、约束或一般要求。",
  "llm.prediction.systemPrompt/subTitle": "AI 指南",
  "llm.prediction.temperature/title": "温度",
  "llm.prediction.temperature/subTitle": "引入多少随机性。0 将始终产生相同的结果，而较高值将增加创造性和变化。",
  "llm.prediction.temperature/info": "来自 llama.cpp 帮助文档：“默认值为 <{{dynamicValue}}>，它在随机性和确定性之间提供了平衡。极端情况下，温度为 0 会始终选择最可能的下一个 token，导致每次运行的输出相同”",
  "llm.prediction.llama.sampling/title": "采样",
  "llm.prediction.topKSampling/title": "Top K 采样",
  "llm.prediction.topKSampling/subTitle": "将下一个 token 限制为模型预测的前 k 个最可能的 token。作用类似于温度",
  "llm.prediction.topKSampling/info": "来自 llama.cpp 帮助文档：\n\nTop-k 采样是一种仅从模型预测的前 k 个最可能的 token 中选择下一个 token 的文本生成方法。\n\n它有助于减少生成低概率或无意义 token 的风险，但也可能限制输出的多样性。\n\n更高的 top-k 值（例如，100）将考虑更多 token，从而生成更多样化的文本，而较低的值（例如，10）将专注于最可能的 token，生成更保守的文本。\n\n• 默认值为 <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "CPU 线程",
  "llm.prediction.llama.cpuThreads/subTitle": "推理期间使用的 CPU 线程数",
  "llm.prediction.llama.cpuThreads/info": "计算期间要使用的线程数。增加线程数并不总是与更好的性能相关联。默认值为 <{{dynamicValue}}>。",
  "llm.prediction.maxPredictedTokens/title": "限制响应长度",
  "llm.prediction.maxPredictedTokens/subTitle": "可选择限制 AI 响应的长度",
  "llm.prediction.maxPredictedTokens/info": "控制聊天机器人响应的最大长度。打开以设置响应的最大长度限制，或关闭以让聊天机器人自行决定何时停止。",
  "llm.prediction.maxPredictedTokens/inputLabel": "最大响应长度（token）",
  "llm.prediction.maxPredictedTokens/wordEstimate": "大约 {{maxWords}} 个词",
  "llm.prediction.repeatPenalty/title": "重复惩罚",
  "llm.prediction.repeatPenalty/subTitle": "对重复相同 token 的行为进行惩罚的程度",
  "llm.prediction.repeatPenalty/info": "来自 llama.cpp 帮助文档：“帮助防止模型生成重复或单调的文本。\n\n较高的值（如 1.5）将更严厉地惩罚重复，而较低的值（如 0.9）将更宽容。” • 默认值为 <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "最小 P 采样",
  "llm.prediction.minPSampling/subTitle": "选择输出 token 的最小基础概率",
  "llm.prediction.minPSampling/info": "来自 llama.cpp 帮助文档：\n\n相对于最可能 token 的概率，一个 token 被考虑的最小概率。必须在 [0, 1] 范围内。\n\n• 默认值为 <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Top P 采样",
  "llm.prediction.topPSampling/subTitle": "可能的下一个 token 的最小累积概率。作用类似于温度",
  "llm.prediction.topPSampling/info": "来自 llama.cpp 帮助文档：\n\nTop-p 采样，也称为核采样，是另一种文本生成方法，它从累积概率至少为 p 的 token 子集中选择下一个 token。\n\n这种方法通过同时考虑 token 的概率和要采样的 token 数量，在多样性和质量之间提供平衡。\n\n较高的 top-p 值（如 0.95）将产生更多样化的文本，而较低的值（如 0.5）将生成更集中和保守的文本。必须在 (0, 1] 范围内。\n\n• 默认值为 <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "停止字符串",
  "llm.prediction.stopStrings/subTitle": "应该停止模型生成更多 token 的字符串",
  "llm.prediction.stopStrings/info": "遇到特定字符串时将停止模型生成更多 token ",
  "llm.prediction.stopStrings/placeholder": "输入字符串并按 ⏎",
  "llm.prediction.contextOverflowPolicy/title": "上下文溢出",
  "llm.prediction.contextOverflowPolicy/subTitle": "当对话变得太大而模型无法处理时的行为方式",
  "llm.prediction.contextOverflowPolicy/info": "决定当对话超出模型工作内存（'上下文'）大小时要做什么",
  "llm.prediction.llama.frequencyPenalty/title": "频率惩罚",
  "llm.prediction.llama.presencePenalty/title": "存在惩罚",
  "llm.prediction.llama.tailFreeSampling/title": "无尾采样",
  "llm.prediction.llama.locallyTypicalSampling/title": "局部典型采样",
  "llm.prediction.llama.xtcProbability/title": "XTC 采样概率",
  "llm.prediction.llama.xtcProbability/subTitle": "XTC（排除顶部选择）采样器将仅以此概率激活每个生成的 token。XTC 采样可以提高创造性并减少陈词滥调",
  "llm.prediction.llama.xtcProbability/info": "XTC（排除顶部选择）采样将仅以此概率激活，每个生成的 token。XTC 采样通常可以提高创造性并减少陈词滥调",
  "llm.prediction.llama.xtcThreshold/title": "XTC 采样阈值",
  "llm.prediction.llama.xtcThreshold/subTitle": "XTC（排除顶部选择）阈值。以 `xtc-probability` 的概率，搜索概率在 `xtc-threshold` 和 0.5 之间的 token，并移除除最不可能的 token 之外的所有 token",
  "llm.prediction.llama.xtcThreshold/info": "XTC（排除顶部选择）阈值。以 `xtc-probability` 的概率，搜索概率在 `xtc-threshold` 和 0.5 之间的 token，并移除除最不可能的 token 之外的所有 token",
  "llm.prediction.mlx.topKSampling/title": "Top K 采样",
  "llm.prediction.mlx.topKSampling/subTitle": "将下一个 token 限制为概率最高的前 k 个 token 之一。作用类似于温度",
  "llm.prediction.mlx.topKSampling/info": "将下一个 token 限制为概率最高的前 k 个 token 之一。作用类似于温度",
  "llm.prediction.onnx.topKSampling/title": "Top K 采样",
  "llm.prediction.onnx.topKSampling/subTitle": "将下一个 token 限制为概率最高的前 k 个 token 之一。作用类似于温度",
  "llm.prediction.onnx.topKSampling/info": "来自 ONNX 文档：\n\n要保留用于 top-k 过滤的最高概率词汇表 token 的数量\n\n• 此过滤器默认关闭",
  "llm.prediction.onnx.repeatPenalty/title": "重复惩罚",
  "llm.prediction.onnx.repeatPenalty/subTitle": "对重复相同 token 的行为进行惩罚的程度",
  "llm.prediction.onnx.repeatPenalty/info": "较高的值会降低模型重复自身的可能性",
  "llm.prediction.onnx.topPSampling/title": "Top P 采样",
  "llm.prediction.onnx.topPSampling/subTitle": "可能的下一个 token 的最小累积概率。作用类似于温度",
  "llm.prediction.onnx.topPSampling/info": "来自 ONNX 文档：\n\n仅保留概率总和达到或超过 TopP 的最可能 token 用于生成\n\n• 此过滤器默认关闭",
  "llm.prediction.seed/title": "种子",
  "llm.prediction.structured/title": "结构化输出",
  "llm.prediction.structured/info": "结构化输出",
  "llm.prediction.structured/description": "高级：您可以提供一个 [JSON Schema](https://json-schema.org/learn/miscellaneous-examples) 来强制模型输出特定格式。阅读[文档](https://lmstudio.ai/docs/advanced/structured-output)了解更多",
  "llm.prediction.tools/title": "工具使用",
  "llm.prediction.tools/description": "高级：您可以提供一个符合 JSON 的工具列表供模型请求调用。阅读[文档](https://lmstudio.ai/docs/advanced/tool-use)了解更多",
  "llm.prediction.tools/serverPageDescriptionAddon": "使用服务器 API 时，将此作为 `tools` 传递到请求体中",
  "llm.prediction.promptTemplate/title": "提示模板",
  "llm.prediction.promptTemplate/subTitle": "聊天中的消息发送给模型的格式。更改此项可能会导致意外行为 - 请确保您知道自己在做什么！",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/title": "要生成的草稿 token 数",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/subTitle": "每个主模型 token 由草稿模型生成的 token 数。找到计算与收益之间的最佳平衡点",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/title": "草稿概率截止值",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/subTitle": "继续草稿直到 token 的概率低于此阈值。较高的值通常意味着较低的风险和收益",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/title": "最小草稿大小",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/subTitle": "小于此值的草稿将被主模型忽略。较高的值通常意味着较低的风险和收益",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/title": "最大草稿大小",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/subTitle": "草稿中允许的最大 token 数。如果所有 token 概率都大于截止值，则为上限。较低的值通常意味着较低的风险和收益",
  "llm.prediction.speculativeDecoding.draftModel/title": "草稿模型",
  "llm.prediction.reasoning.parsing/title": "推理部分解析",
  "llm.prediction.reasoning.parsing/subTitle": "如何解析模型输出中的推理部分",

  "llm.load.contextLength/title": "上下文长度",
  "llm.load.contextLength/subTitle": "模型在一个提示中可以关注的最大 token 数。查看“推理参数”下的对话溢出选项以获取更多管理方法",
  "llm.load.contextLength/info": "指定模型一次可以考虑的最大 token 数，影响其在处理过程中保留的上下文量",
  "llm.load.contextLength/warning": "设置较高的上下文长度值可能会显著影响内存使用",
  "llm.load.seed/title": "种子",
  "llm.load.seed/subTitle": "用于文本生成的随机数生成器的种子。-1 表示随机",
  "llm.load.seed/info": "随机种子：设置随机数生成的种子以确保结果可重现",

  "llm.load.llama.evalBatchSize/title": "评估批次大小",
  "llm.load.llama.evalBatchSize/subTitle": "一次处理的输入 token 数。增加此值会提高性能但会增加内存使用",
  "llm.load.llama.evalBatchSize/info": "设置评估期间一起处理的示例数量，影响速度和内存使用",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE 频率基数",
  "llm.load.llama.ropeFrequencyBase/subTitle": "旋转位置编码（RoPE）的自定义基础频率。增加此值可能在高上下文长度时实现更好的性能",
  "llm.load.llama.ropeFrequencyBase/info": "[高级] 调整旋转位置编码的基础频率，影响位置信息的嵌入方式",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE 频率缩放",
  "llm.load.llama.ropeFrequencyScale/subTitle": "上下文长度通过此因子缩放以使用 RoPE 扩展有效上下文",
  "llm.load.llama.ropeFrequencyScale/info": "[高级] 修改旋转位置编码的频率缩放以控制位置编码的粒度",
  "llm.load.llama.acceleration.offloadRatio/title": "GPU 卸载",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "用于 GPU 加速的在 GPU 上计算的离散模型层数",
  "llm.load.llama.acceleration.offloadRatio/info": "设置要卸载到 GPU 的层数。",
  "llm.load.llama.flashAttention/title": "闪电注意力",
  "llm.load.llama.flashAttention/subTitle": "在某些模型上减少内存使用和生成时间",
  "llm.load.llama.flashAttention/info": "加速注意力机制以实现更快和更高效的处理",
  "llm.load.numExperts/title": "专家数量",
  "llm.load.numExperts/subTitle": "在模型中使用的专家数量",
  "llm.load.numExperts/info": "在模型中使用的专家数量",
  "llm.load.llama.keepModelInMemory/title": "保持模型在内存中",
  "llm.load.llama.keepModelInMemory/subTitle": "即使卸载到 GPU，也为模型保留系统内存。提高性能但需要更多系统 RAM",
  "llm.load.llama.keepModelInMemory/info": "防止模型被换出到磁盘，以更高的 RAM 使用为代价确保更快的访问",
  "llm.load.llama.useFp16ForKVCache/title": "KV 缓存使用 FP16",
  "llm.load.llama.useFp16ForKVCache/info": "通过以半精度（FP16）存储缓存来减少内存使用",
  "llm.load.llama.tryMmap/title": "尝试 mmap()",
  "llm.load.llama.tryMmap/subTitle": "改善模型加载时间。禁用此项可能在模型大于可用系统 RAM 时提高性能",
  "llm.load.llama.tryMmap/info": "直接从磁盘将模型文件加载到内存",
  "llm.load.llama.cpuThreadPoolSize/title": "CPU 线程池大小",
  "llm.load.llama.cpuThreadPoolSize/subTitle": "分配给用于模型计算的线程池的 CPU 线程数",
  "llm.load.llama.cpuThreadPoolSize/info": "分配给用于模型计算的线程池的 CPU 线程数。增加线程数并不总是与更好的性能相关联。默认值为 <{{dynamicValue}}>。",
  "llm.load.llama.kCacheQuantizationType/title": "K 缓存量化类型",
  "llm.load.llama.kCacheQuantizationType/subTitle": "较低的值减少内存使用但可能降低质量。效果在不同模型之间差异显著。",
  "llm.load.llama.vCacheQuantizationType/title": "V 缓存量化类型",
  "llm.load.llama.vCacheQuantizationType/subTitle": "较低的值减少内存使用但可能降低质量。效果在不同模型之间差异显著。",
  "llm.load.llama.vCacheQuantizationType/turnedOnWarning": "⚠️ 如果未启用闪电注意力，您必须禁用此值",
  "llm.load.llama.vCacheQuantizationType/disabledMessage": "只有在启用闪电注意力时才能打开",
  "llm.load.llama.vCacheQuantizationType/invalidF32MetalState": "⚠️ 使用 F32 时必须禁用闪电注意力",
  "llm.load.mlx.kvCacheBits/title": "KV 缓存量化",
  "llm.load.mlx.kvCacheBits/subTitle": "KV 缓存应该量化到的位数",
  "llm.load.mlx.kvCacheBits/info": "KV 缓存应该量化到的位数",
  "llm.load.mlx.kvCacheBits/turnedOnWarning": "使用 KV 缓存量化时将忽略上下文长度设置",
  "llm.load.mlx.kvCacheGroupSize/title": "KV 缓存量化：组大小",
  "llm.load.mlx.kvCacheGroupSize/subTitle": "KV 缓存量化操作期间的组大小。较高的组大小减少内存使用但可能降低质量",
  "llm.load.mlx.kvCacheGroupSize/info": "KV 缓存应该量化到的位数",
  "llm.load.mlx.kvCacheQuantizationStart/title": "KV 缓存量化：当上下文超过此长度时开始量化",
  "llm.load.mlx.kvCacheQuantizationStart/subTitle": "开始量化 KV 缓存的上下文长度阈值",
  "llm.load.mlx.kvCacheQuantizationStart/info": "开始量化 KV 缓存的上下文长度阈值",
  "llm.load.mlx.kvCacheQuantization/title": "KV 缓存量化",
  "llm.load.mlx.kvCacheQuantization/subTitle": "量化模型的 KV 缓存。这可能会导致更快的生成速度和更低的内存占用，但可能会影响模型输出的质量。",
  "llm.load.mlx.kvCacheQuantization/bits/title": "KV 缓存量化位数",
  "llm.load.mlx.kvCacheQuantization/bits/tooltip": "将 KV 缓存量化到的位数",
  "llm.load.mlx.kvCacheQuantization/bits/bits": "位",
  "llm.load.mlx.kvCacheQuantization/groupSize/title": "组大小策略",
  "llm.load.mlx.kvCacheQuantization/groupSize/accuracy": "准确性",
  "llm.load.mlx.kvCacheQuantization/groupSize/balanced": "平衡",
  "llm.load.mlx.kvCacheQuantization/groupSize/speedy": "快速",
  "llm.load.mlx.kvCacheQuantization/groupSize/tooltip": "高级：量化的'矩阵乘法组大小'配置\n\n• 准确性 = 组大小 32\n• 平衡 = 组大小 64\n• 快速 = 组大小 128\n",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/title": "当上下文达到此长度时开始量化",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/tooltip": "当上下文达到这么多 token 时，开始量化 KV 缓存",

  "embedding.load.contextLength/title": "上下文长度",
  "embedding.load.contextLength/subTitle": "模型在一个提示中可以关注的最大 token 数。查看“推理参数”下的对话溢出选项以获取更多管理方法",
  "embedding.load.contextLength/info": "指定模型一次可以考虑的最大 token 数，影响其在处理过程中保留的上下文量",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE 频率基数",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "旋转位置编码（RoPE）的自定义基础频率。增加此值可能在高上下文长度时实现更好的性能",
  "embedding.load.llama.ropeFrequencyBase/info": "[高级] 调整旋转位置编码的基础频率，影响位置信息的嵌入方式",
  "embedding.load.llama.evalBatchSize/title": "评估批次大小",
  "embedding.load.llama.evalBatchSize/subTitle": "一次处理的输入 token 数。增加此值会提高性能但会增加内存使用",
  "embedding.load.llama.evalBatchSize/info": "设置评估期间一起处理的 token 数",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE 频率缩放",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "上下文长度通过此因子缩放以使用 RoPE 扩展有效上下文",
  "embedding.load.llama.ropeFrequencyScale/info": "[高级] 修改旋转位置编码的频率缩放以控制位置编码的粒度",
  "embedding.load.llama.acceleration.offloadRatio/title": "GPU 卸载",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "用于 GPU 加速的在 GPU 上计算的离散模型层数",
  "embedding.load.llama.acceleration.offloadRatio/info": "设置要卸载到 GPU 的层数。",
  "embedding.load.llama.keepModelInMemory/title": "保持模型在内存中",
  "embedding.load.llama.keepModelInMemory/subTitle": "即使卸载到 GPU，也为模型保留系统内存。提高性能但需要更多系统 RAM",
  "embedding.load.llama.keepModelInMemory/info": "防止模型被换出到磁盘，以更高的 RAM 使用为代价确保更快的访问",
  "embedding.load.llama.tryMmap/title": "尝试 mmap()",
  "embedding.load.llama.tryMmap/subTitle": "改善模型加载时间。禁用此项可能在模型大于可用系统 RAM 时提高性能",
  "embedding.load.llama.tryMmap/info": "直接从磁盘将模型文件加载到内存",
  "embedding.load.seed/title": "种子",
  "embedding.load.seed/subTitle": "用于文本生成的随机数生成器的种子。-1 表示随机种子",
  "embedding.load.seed/info": "随机种子：设置随机数生成的种子以确保结果可重现",

  "presetTooltip": {
    "included/title": "预设值",
    "included/description": "以下字段将被应用",
    "included/empty": "此预设在当前上下文中没有可应用的字段。",
    "included/conflict": "您将被询问是否应用此值",
    "separateLoad/title": "加载时配置",
    "separateLoad/description.1": "预设还包含以下加载时配置。加载时配置是模型范围的，需要重新加载模型才能生效。按住",
    "separateLoad/description.2": "以应用到",
    "separateLoad/description.3": "。",
    "excluded/title": "可能不适用",
    "excluded/description": "预设包含以下字段，但在当前上下文中不适用。",
    "legacy/title": "旧版预设",
    "legacy/description": "这是一个旧版预设。它包含以下字段，这些字段现在要么是自动处理的，要么不再适用。",
    "button/publish": "发布到 Hub",
    "button/pushUpdate": "推送更改到 Hub",
    "button/export": "导出"
  },

  "customInputs": {
    "string": {
      "emptyParagraph": "<空>"
    },
    "checkboxNumeric": {
      "off": "关闭"
    },
    "llamaCacheQuantizationType": {
      "off": "关闭"
    },
    "mlxKvCacheBits": {
      "off": "关闭"
    },
    "stringArray": {
      "empty": "<空>"
    },
    "llmPromptTemplate": {
      "type": "类型",
      "types.jinja/label": "模板 (Jinja)",
      "jinja.bosToken/label": "BOS 标记",
      "jinja.eosToken/label": "EOS 标记",
      "jinja.template/label": "模板",
      "jinja/error": "解析 Jinja 模板失败：{{error}}",
      "jinja/empty": "请在上方输入 Jinja 模板。",
      "jinja/unlikelyToWork": "您提供的 Jinja 模板可能无法工作，因为它没有引用变量“messages”。请检查您是否输入了正确的模板。",
      "types.manual/label": "手动",
      "manual.subfield.beforeSystem/label": "系统前缀",
      "manual.subfield.beforeSystem/placeholder": "输入系统前缀...",
      "manual.subfield.afterSystem/label": "系统后缀",
      "manual.subfield.afterSystem/placeholder": "输入系统后缀...",
      "manual.subfield.beforeUser/label": "用户前缀",
      "manual.subfield.beforeUser/placeholder": "输入用户前缀...",
      "manual.subfield.afterUser/label": "用户后缀",
      "manual.subfield.afterUser/placeholder": "输入用户后缀...",
      "manual.subfield.beforeAssistant/label": "助手前缀",
      "manual.subfield.beforeAssistant/placeholder": "输入助手前缀...",
      "manual.subfield.afterAssistant/label": "助手后缀",
      "manual.subfield.afterAssistant/placeholder": "输入助手后缀...",
      "stopStrings/label": "额外停止字符串",
      "stopStrings/subTitle": "模板特定的停止字符串，将与用户指定的停止字符串一起使用。"
    },
    "contextLength": {
      "maxValueTooltip": "这是模型训练时能处理的最大 token 数。点击设置上下文为此值",
      "maxValueTextStart": "模型最多支持",
      "maxValueTextEnd": "个 token",
      "tooltipHint": "虽然模型可能支持到特定数量的 token，但如果您的机器资源无法处理负载，性能可能会下降 - 增加此值时请谨慎"
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "在限制处停止",
      "stopAtLimitSub": "当模型的内存已满时停止生成",
      "truncateMiddle": "截断中间部分",
      "truncateMiddleSub": "从对话中间删除消息以为新消息腾出空间。模型仍会记住对话的开始部分",
      "rollingWindow": "滚动窗口",
      "rollingWindowSub": "模型将始终获得最近的几条消息，但可能会忘记对话的开始部分"
    },
    "llamaAccelerationOffloadRatio": {
      "max": "MAX",
      "off": "OFF"
    },
    "llamaAccelerationSplitStrategy": {
      "evenly": "平均分配",
      "favorMainGpu": "优先主 GPU"
    },
    "speculativeDecodingDraftModel": {
      "readMore": "了解工作原理",
      "placeholder": "选择一个兼容的草稿模型",
      "noCompatible": "未找到与当前模型选择兼容的草稿模型",
      "stillLoading": "正在识别兼容的草稿模型...",
      "notCompatible": "所选的草稿模型 (<draft/>) 与当前模型选择 (<current/>) 不兼容。",
      "off": "关闭",
      "loadModelToSeeOptions": "加载模型 <keyboard-shortcut /> 以查看兼容选项",
      "compatibleWithNumberOfModels": "推荐用于至少 {{dynamicValue}} 个模型",
      "recommendedForSomeModels": "推荐用于某些模型",
      "recommendedForLlamaModels": "推荐用于 Llama 模型",
      "recommendedForQwenModels": "推荐用于 Qwen 模型",
      "onboardingModal": {
        "introducing": "介绍",
        "speculativeDecoding": "推测性解码",
        "firstStepBody": "适用于 <custom-span>llama.cpp</custom-span> 和 <custom-span>MLX</custom-span> 模型的推理加速",
        "secondStepTitle": "使用推测性解码进行推理加速",
        "secondStepBody": "推测性解码是一种涉及两个模型协作的技术：\n - 一个较大的“主”模型\n - 一个较小的“草稿”模型\n\n在生成过程中，草稿模型快速提出 token 供较大的主模型验证。验证 token 比实际生成它们要快得多，这是速度提升的来源。**通常，主模型和草稿模型之间的大小差异越大，速度提升就越显著**。\n\n为了保持质量，主模型只接受与其本身将生成的内容一致的 token，从而在更快的推理速度下实现较大模型的响应质量。两个模型必须共享相同的词汇表。",
        "draftModelRecommendationsTitle": "草稿模型推荐",
        "basedOnCurrentModels": "基于您当前的模型",
        "close": "关闭",
        "next": "下一步",
        "done": "完成"
      },
      "speculativeDecodingLoadModelToSeeOptions": "请先加载模型 <model-badge />",
      "errorEngineNotSupported": "推测性解码需要引擎 {{engineName}} 的 {{minVersion}} 或更高版本。请更新引擎 (<key/>) 并重新加载模型以使用此功能。",
      "errorEngineNotSupported/noKey": "推测性解码需要引擎 {{engineName}} 的 {{minVersion}} 或更高版本。请更新引擎并重新加载模型以使用此功能。"
    },
    "llmReasoningParsing": {
      "startString/label": "开始字符串",
      "startString/placeholder": "输入开始字符串...",
      "endString/label": "结束字符串",
      "endString/placeholder": "输入结束字符串..."
    }
  },
  "saveConflictResolution": {
    "title": "选择要包含在预设中的值",
    "description": "选择要保留的值",
    "instructions": "点击一个值以包含它",
    "userValues": "之前的值",
    "presetValues": "新值",
    "confirm": "确认",
    "cancel": "取消"
  },
  "applyConflictResolution": {
    "title": "保留哪些值？",
    "description": "您有未提交的更改与传入的预设重叠",
    "instructions": "点击一个值以保留它",
    "userValues": "当前值",
    "presetValues": "传入的预设值",
    "confirm": "确认",
    "cancel": "取消"
  },
  "empty": "<空>",
  "noModelSelected": "未选择模型",
  "apiIdentifier.label": "API 标识符",
  "apiIdentifier.hint": "可选择为此模型提供一个标识符。这将在 API 请求中使用。留空以使用默认标识符。",
  "idleTTL.label": "空闲自动卸载（TTL）",
  "idleTTL.hint": "如果设置，模型在空闲指定时间后将自动卸载。",
  "idleTTL.mins": "分钟",

  "presets": {
    "title": "预设",
    "commitChanges": "提交更改",
    "commitChanges/description": "将您的更改提交到预设。",
    "commitChanges.manual": "检测到新字段。您可以选择要包含在预设中的更改。",
    "commitChanges.manual.hold.0": "按住",
    "commitChanges.manual.hold.1": "以选择要提交到预设的更改。",
    "commitChanges.saveAll.hold.0": "按住",
    "commitChanges.saveAll.hold.1": "以保存所有更改。",
    "commitChanges.saveInPreset.hold.0": "按住",
    "commitChanges.saveInPreset.hold.1": "以仅保存已包含在预设中的字段的更改。",
    "commitChanges/error": "提交更改到预设失败。",
    "commitChanges.manual/description": "选择要包含在预设中的更改。",
    "saveAs": "另存为新预设...",
    "presetNamePlaceholder": "输入预设名称...",
    "cannotCommitChangesLegacy": "这是一个旧版预设，无法修改。您可以使用“另存为新预设...”创建副本。",
    "cannotCommitChangesNoChanges": "没有要提交的更改。",
    "emptyNoUnsaved": "选择预设...",
    "emptyWithUnsaved": "未保存的预设",
    "saveEmptyWithUnsaved": "将预设另存为...",
    "saveConfirm": "保存",
    "saveCancel": "取消",
    "saving": "保存中...",
    "save/error": "保存预设失败。",
    "deselect": "取消选择预设",
    "deselect/error": "取消选择预设失败。",
    "select/error": "选择预设失败。",
    "delete/error": "删除预设失败。",
    "discardChanges": "放弃未保存",
    "discardChanges/info": "放弃所有未提交的更改并将预设恢复到原始状态",
    "newEmptyPreset": "+ 新建预设",
    "importPreset": "导入",
    "contextMenuSelect": "应用预设",
    "contextMenuDelete": "删除...",
    "contextMenuShare": "发布...",
    "contextMenuOpenInHub": "在 Hub 中查看",
    "contextMenuPushChanges": "推送更改到 Hub",
    "contextMenuPushingChanges": "推送中...",
    "contextMenuPushedChanges": "更改已推送",
    "contextMenuExport": "导出文件",
    "contextMenuRevealInExplorer": "在文件资源管理器中显示",
    "contextMenuRevealInFinder": "在访达中显示",
    "share": {
      "title": "发布预设",
      "action": "分享您的预设供他人下载、点赞和复刻",
      "presetOwnerLabel": "所有者",
      "uploadAs": "您的预设将创建为 {{name}}",
      "presetNameLabel": "预设名称",
      "descriptionLabel": "描述（可选）",
      "loading": "发布中...",
      "success": "预设已成功推送",
      "presetIsLive": "<preset-name /> 现已在 Hub 上线！",
      "close": "关闭",
      "confirmViewOnWeb": "在网页上查看",
      "confirmCopy": "复制 URL",
      "confirmCopied": "已复制！",
      "pushedToHub": "您的预设已推送到 Hub",
      "descriptionPlaceholder": "输入描述...",
      "willBePublic": "发布您的预设将使其公开",
      "publicSubtitle": "您的预设是<custom-bold>公开</custom-bold>的。其他人可以在 lmstudio.ai 上下载和复刻它",
      "confirmShareButton": "发布",
      "error": "发布预设失败",
      "createFreeAccount": "创建一个免费账号以在 Hub 上发布预设"
    },
    "update": {
      "title": "推送更改到 Hub",
      "title/success": "预设已成功更新",
      "subtitle": "对 <custom-preset-name /> 进行更改并推送到 Hub",
      "descriptionLabel": "描述",
      "descriptionPlaceholder": "输入描述...",
      "loading": "推送中...",
      "cancel": "取消",
      "createFreeAccount": "创建一个免费账号以在 Hub 上发布预设",
      "error": "推送更新失败",
      "confirmUpdateButton": "推送"
    },
    "import": {
      "title": "从文件导入预设",
      "dragPrompt": "拖放预设 JSON 文件或<custom-link>从您的计算机选择</custom-link>",
      "remove": "移除",
      "cancel": "取消",
      "importPreset_zero": "导入预设",
      "importPreset_one": "导入预设",
      "importPreset_other": "导入 {{count}} 个预设",
      "selectDialog": {
        "title": "选择预设文件（.json）",
        "button": "导入"
      },
      "error": "导入预设失败",
      "resultsModal": {
        "titleSuccessSection_one": "成功导入 1 个预设",
        "titleSuccessSection_other": "成功导入 {{count}} 个预设",
        "titleFailSection_zero": "",
        "titleFailSection_one": "（{{count}} 个失败）",
        "titleFailSection_other": "（{{count}} 个失败）",
        "titleAllFailed": "导入预设失败",
        "importMore": "导入更多",
        "close": "完成",
        "successBadge": "成功",
        "alreadyExistsBadge": "预设已存在",
        "errorBadge": "错误",
        "invalidFileBadge": "无效文件",
        "otherErrorBadge": "导入预设失败",
        "errorViewDetailsButton": "查看详情",
        "seeError": "查看错误",
        "noName": "无预设名称",
        "useInChat": "在聊天中使用"
      },
      "importFromUrl": {
        "button": "从 URL 导入...",
        "title": "从 URL 导入",
        "back": "从文件导入...",
        "action": "在下方粘贴要导入的预设的 LM Studio Hub URL",
        "invalidUrl": "无效的 URL。请确保您粘贴的是正确的 LM Studio Hub URL。",
        "tip": "您可以在 LM Studio Hub 中使用 {{buttonName}} 按钮直接安装预设",
        "confirm": "导入",
        "cancel": "取消",
        "loading": "导入中...",
        "error": "下载预设失败。"
      }
    },
    "download": {
      "title": "从 LM Studio Hub 拉取 <preset-name />",
      "subtitle": "保存 <custom-name /> 到您的预设。这样做将允许您在应用中使用此预设",
      "button": "拉取",
      "button/loading": "拉取中...",
      "cancel": "取消",
      "error": "下载预设失败。"
    },
    "inclusiveness": {
      "speculativeDecoding": "包含在预设中"
    }
  },

  "flashAttentionWarning": "闪电注意力是一个实验性功能，可能会导致某些模型出现问题。如果遇到问题，请尝试禁用它。",
  "llamaKvCacheQuantizationWarning": "KV 缓存量化是一个实验性功能，可能会导致某些模型出现问题。V 缓存量化需要启用闪电注意力。如果遇到问题，请重置为默认值“F16”。",

  "seedUncheckedHint": "随机种子",
  "ropeFrequencyBaseUncheckedHint": "自动",
  "ropeFrequencyScaleUncheckedHint": "自动",

  "hardware": {
    "advancedGpuSettings": "高级 GPU 设置",
    "advancedGpuSettings.info": "如果不确定，请保持默认值",
    "advancedGpuSettings.reset": "重置为默认值",
    "environmentVariables": {
      "title": "环境变量",
      "description": "模型生命周期内的活动环境变量。",
      "key.placeholder": "选择变量...",
      "value.placeholder": "值"
    },
    "mainGpu": {
      "title": "主 GPU",
      "description": "优先用于模型计算的 GPU。",
      "placeholder": "选择主 GPU..."
    },
    "splitStrategy": {
      "title": "分割策略",
      "description": "如何在多个 GPU 之间分配模型计算。",
      "placeholder": "选择分割策略..."
    }
  }
}
